
---


> Most of these notes are provided by Microsoft [here](https://quantum.microsoft.com/en-us/experience/quantum-katas) 
> Another good source that details much found on here can be found on [this page](https://en.wikipedia.org/wiki/Determinant)

AÂ **matrix**Â is set of numbers arranged in a rectangular grid. Here is aÂ 2Â byÂ 2Â matrix:
$$A =
\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$$
The notation $A_{i,j}$Â refers to the element in rowÂ ğ‘–Â and columnÂ ğ‘—Â of matrixÂ ğ´Â (all indices are 0-based). In the above example,Â $A_{0,1} = 2$.

AnÂ ğ‘›Ã—ğ‘šÂ matrix will haveÂ ğ‘›Â rows andÂ ğ‘šÂ columns:

$$\begin{bmatrix}
    x_{0,0} & x_{0,1} & \dotsb & x_{0,m-1} \\
    x_{1,0} & x_{1,1} & \dotsb & x_{1,m-1} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    x_{n-1,0} & x_{n-1,1} & \dotsb & x_{n-1,m-1}
\end{bmatrix}$$

AÂ 1Ã—1Â matrix is equivalent to a scalar:
$$\begin{bmatrix} 3 \end{bmatrix} = 3$$
Quantum computing uses complex-valued matrices: the elements of a matrix can be complex numbers. This, for example, is a valid complex-valued matrix:$$\begin{bmatrix}
    1 & i \\
    -2i & 3 + 4i
\end{bmatrix}$$
Finally, aÂ **vector**Â is anÂ ğ‘›Ã—1Â matrix. Here, for example, is aÂ 3Ã—1Â vector:$$V = \begin{bmatrix} 1 \\ 2i \\ 3 + 4i \end{bmatrix}$$
Since vectors always have a width ofÂ 1, vector elements are sometimes written using only one index. In the above example, $V_0 = 1$Â andÂ $V_1 = 2i$.


### Matrix Addition
---

$$\begin{bmatrix}
    x_{0,0} & x_{0,1} & \dotsb & x_{0,m-1} \\
    x_{1,0} & x_{1,1} & \dotsb & x_{1,m-1} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    x_{n-1,0} & x_{n-1,1} & \dotsb & x_{n-1,m-1}
\end{bmatrix} +$$
$$+ \begin{bmatrix}
    y_{0,0} & y_{0,1} & \dotsb & y_{0,m-1} \\
    y_{1,0} & y_{1,1} & \dotsb & y_{1,m-1} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    y_{n-1,0} & y_{n-1,1} & \dotsb & y_{n-1,m-1}
\end{bmatrix} =$$
$$= \begin{bmatrix}
    x_{0,0} + y_{0,0} & x_{0,1} + y_{0,1} & \dotsb & x_{0,m-1} + y_{0,m-1} \\
    x_{1,0} + y_{1,0} & x_{1,1} + y_{1,1} & \dotsb & x_{1,m-1} + y_{1,m-1} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    x_{n-1,0} + y_{n-1,0} & x_{n-1,1} + y_{n-1,1} & \dotsb & x_{n-1,m-1} + y_{n-1,m-1}
\end{bmatrix}$$

Similarly, we can computeÂ ğ´âˆ’ğµÂ by subtracting elements ofÂ ğµÂ from corresponding elements ofÂ ğ´.

Matrix addition has the following properties:

- Commutativity:Â ğ´+ğµ=ğµ+ğ´
- Associativity:Â (ğ´+ğµ)+ğ¶=ğ´+(ğµ+ğ¶)

### Scalar Multiplication
---

The next matrix operation isÂ **scalar multiplication**Â - multiplying the entire matrix by a scalar (real or complex number):

$$a \cdot
\begin{bmatrix}
    x_{0,0} & x_{0,1} & \dotsb & x_{0,m-1} \\
    x_{1,0} & x_{1,1} & \dotsb & x_{1,m-1} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    x_{n-1,0} & x_{n-1,1} & \dotsb & x_{n-1,m-1}
\end{bmatrix} =
\begin{bmatrix}
    a \cdot x_{0,0} & a \cdot x_{0,1} & \dotsb & a \cdot x_{0,m-1} \\
    a \cdot x_{1,0} & a \cdot x_{1,1} & \dotsb & a \cdot x_{1,m-1} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    a \cdot x_{n-1,0} & a \cdot x_{n-1,1} & \dotsb & a \cdot x_{n-1,m-1}
\end{bmatrix}$$

Scalar multiplication has the following properties:

- Associativity:Â ğ‘¥â‹…(ğ‘¦ğ´)=(ğ‘¥â‹…ğ‘¦)ğ´
- Distributivity over matrix addition:Â ğ‘¥(ğ´+ğµ)=ğ‘¥ğ´+ğ‘¥ğµ
- Distributivity over scalar addition:

### Matrix Multiplication
---

**Matrix multiplication**Â is a very important and somewhat unusual operation. The unusual thing about it is that neither its operands nor its output are the same size: anÂ ğ‘›Ã—ğ‘šÂ matrix multiplied by anÂ ğ‘šÃ—ğ‘˜Â matrix results in anÂ ğ‘›Ã—ğ‘˜Â matrix. That is, for matrix multiplication to be applicable, the number of columns in the first matrix must equal the number of rows in the second matrix.

Here is how matrix product is calculated: if we are calculatingÂ ğ´ğµ=ğ¶, then

$$C_{i,j} = A_{i,0} \cdot B_{0,j} + A_{i,1} \cdot B_{1,j} + \dotsb + A_{i,m-1} \cdot B_{m-1,j} = \sum_{t = 0}^{m-1} A_{i,t} \cdot B_{t,j}$$

Here is a small example:

$$\begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6
\end{bmatrix}
\begin{bmatrix}
    1 \\
    2 \\
    3
\end{bmatrix} =
\begin{bmatrix}
    1 \cdot 1 + 2 \cdot 2 + 3 \cdot 3 \\
    4 \cdot 1 + 5 \cdot 2 + 6 \cdot 3
\end{bmatrix} =
\begin{bmatrix}
    14 \\
    32
\end{bmatrix}$$

Matrix multiplication has the following properties:

- Associativity:Â ğ´(ğµğ¶)=(ğ´ğµ)ğ¶
- Distributivity over matrix addition:Â ğ´(ğµ+ğ¶)=ğ´ğµ+ğ´ğ¶Â andÂ (ğ´+ğµ)ğ¶=ğ´ğ¶+ğµğ¶
- Associativity with scalar multiplication:Â ğ‘¥ğ´ğµ=ğ‘¥(ğ´ğµ)=ğ´(ğ‘¥ğµ)

> Note that matrix multiplication isÂ **not commutative:**Â ğ´ğµÂ rarely equalsÂ ğµğ´.

Another very important property of matrix multiplication is that a matrix multiplied by a vector produces another vector.


### Identity Matrix
---

AnÂ **identity matrix**Â ğ¼ğ‘›Â is a specialÂ ğ‘›Ã—ğ‘›Â matrix which hasÂ 1s on the main diagonal, andÂ 0s everywhere else:

$$I_n =
\begin{bmatrix}
    1 & 0 & \dotsb & 0 \\
    0 & 1 & \dotsb & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dotsb & 1
\end{bmatrix}$$

What makes it special is that multiplying any matrix (of compatible size) byÂ $I_n$Â returns the original matrix. To put it another way, ifÂ ğ´Â is anÂ ğ‘›Ã—ğ‘šÂ matrix:

$$AI_m = I_nA = A$$

This is whyÂ $I_n$Â is called an identity matrix - it acts as aÂ **multiplicative identity**. In other words, it is the matrix equivalent of the numberÂ 1.

### Inverse Matrices
---


A squareÂ ğ‘›Ã—ğ‘›Â matrixÂ ğ´Â isÂ **invertible**Â if it has an inverseÂ ğ‘›Ã—ğ‘›Â matrixÂ $A^{-1}$Â with the following property:

$$AA^{-1} = A^{-1}A = I_n$$

In other words,Â $A^{-1}$Â acts as theÂ **multiplicative inverse**Â ofÂ ğ´.

Another, equivalent definition highlights what makes this an interesting property. For any matricesÂ ğµÂ andÂ ğ¶Â of compatible sizes:

$$A^{-1}(AB) = A(A^{-1}B) = B$$
$$(CA)A^{-1} = (CA^{-1})A = C$$

A square matrix has a property called theÂ **determinant**, with the determinant of matrixÂ ğ´Â being written asÂ |ğ´|. A matrix is invertible if and only if its determinant isn't equal toÂ 0.

For aÂ 2Ã—2Â matrixÂ ğ´, the determinant is defined asÂ $|A| = A_{0,0} \cdot A_{1,1} - A_{0,1} \cdot A_{1,0}$.

### Matrix Transposition
---

TheÂ **transpose**Â operation, denoted asÂ $A^T$, is essentially a reflection of the matrix across the diagonal:Â $A^T_{i,j} = A_{j,i}$.

Given anÂ ğ‘›Ã—ğ‘šÂ matrixÂ ğ´, its transpose is theÂ ğ‘šÃ—ğ‘›Â matrixÂ $A^T$, such that if:

$$A =
\begin{bmatrix}
    x_{0,0} & x_{0,1} & \dotsb & x_{0,m-1} \\
    x_{1,0} & x_{1,1} & \dotsb & x_{1,m-1} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{n-1,0} & x_{n-1,1} & \dotsb & x_{n-1,m-1}
\end{bmatrix}$$

then:
$$A^T =
\begin{bmatrix}
    x_{0,0} & x_{1,0} & \dotsb & x_{n-1,0} \\
    x_{0,1} & x_{1,1} & \dotsb & x_{n-1,1} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{0,m-1} & x_{1,m-1} & \dotsb & x_{n-1,m-1}
\end{bmatrix}$$

For example:
$$\begin{bmatrix}
    1 & 2 \\
    3 & 4 \\
    5 & 6
\end{bmatrix}^T =
\begin{bmatrix}
    1 & 3 & 5 \\
    2 & 4 & 6
\end{bmatrix}$$
AÂ **symmetric**Â matrix is a square matrix which equals its own transpose:Â $A = A^T$. To put it another way, it has reflection symmetry (hence the name) across the main diagonal. For example, the following matrix is symmetric

$$\begin{bmatrix}
    1 & 2 & 3 \\
    2 & 4 & 5 \\
    3 & 5 & 6
\end{bmatrix}$$

The transpose of a matrix product is equal to the product of transposed matrices, taken in reverse order:$$(AB)^T = B^TA^T$$

### Matrix Conjugates
---
The next important single-matrix operation is theÂ **matrix conjugate**, denoted asÂ ğ´â€•. This operation makes sense only for complex-valued matrices; as the name might suggest, it involves taking the complex conjugate of every element of the matrix: if

$$A =
\begin{bmatrix}
    x_{0,0} & x_{0,1} & \dotsb & x_{0,m-1} \\
    x_{1,0} & x_{1,1} & \dotsb & x_{1,m-1} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{n-1,0} & x_{n-1,1} & \dotsb & x_{n-1,m-1}
\end{bmatrix}$$

Then:

$$\overline{A} =
\begin{bmatrix}
    \overline{x}_{0,0} & \overline{x}_{0,1} & \dotsb & \overline{x}_{0,m-1} \\
    \overline{x}_{1,0} & \overline{x}_{1,1} & \dotsb & \overline{x}_{1,m-1} \\
    \vdots & \vdots & \ddots & \vdots \\
    \overline{x}_{n-1,0} & \overline{x}_{n-1,1} & \dotsb & \overline{x}_{n-1,m-1}
\end{bmatrix}$$

> As a reminder, a conjugate of a complex numberÂ $x = a + bi$ is $\overline{x} = a - bi$

The conjugate of a matrix product equals to the product of conjugates of the matrices:

$$\overline{AB} = (\overline{A})(\overline{B})$$

### Adjoint Matrix 
---

The final important single-matrix operation is a combination of the previous two. TheÂ **conjugate transpose**, also called theÂ **adjoint**Â of matrixÂ ğ´, is defined asÂ $A^\dagger = \overline{(A^T)} = (\overline{A})^T$.

A matrix is known asÂ **Hermitian**Â orÂ **self-adjoint**Â if it equals its own adjoint:Â $A = A^\dagger$. For example, the following matrix is Hermitian:

$$\begin{bmatrix}
    1 & i \\
    -i & 2
\end{bmatrix}$$
The adjoint of a matrix product can be calculated as follows:

$$(AB)^\dagger = B^\dagger A^\dagger$$

### Unitary Matrices
---

**Unitary matrices**Â are very important for quantum computing. A matrix is unitary when it is invertible, and its inverse is equal to its adjoint:Â $U^{-1} = U^\dagger$. That is, anÂ ğ‘›Ã—ğ‘›Â square matrixÂ ğ‘ˆÂ is unitary if and only ifÂ $UU^\dagger = U^\dagger U = I_n$.

Is this matrix unitary?

$$A = \begin{bmatrix}
    \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
    \frac{i}{\sqrt{2}} & \frac{-i}{\sqrt{2}}
\end{bmatrix} = 
\frac{1}{\sqrt{2}} \begin{bmatrix}
    1 & 1 \\
    i & -i
\end{bmatrix}$$

To check whether the input matrix is unitary, we will need to perform the following steps:

1. Calculate the adjoint of the input matrixÂ $A^\dagger$.

$$A^\dagger = \frac{1}{\sqrt{2}} \begin{bmatrix}
     1 & -i \\
     1 & i
 \end{bmatrix}$$

1. Multiply it by the input matrix.

$$AA^\dagger = \frac12 \begin{bmatrix}
     1 & 1 \\
     i & -i
 \end{bmatrix} \begin{bmatrix}
     1 & -i \\
     1 & i
 \end{bmatrix} = \frac12 \begin{bmatrix}
     1 \cdot 1 + 1 \cdot 1 & 1 \cdot (-i) + 1 \cdot i \\
     i \cdot 1 + (-i) \cdot 1 & i \cdot (-i) + (-i) \cdot i
 \end{bmatrix} = \begin{bmatrix}
     1 & 0 \\
     0 & 1
 \end{bmatrix}$$

If the multiplication resultÂ $AA^\dagger$Â is an identity matrix, which is indeed the case, and the productÂ $A^\dagger A$Â is also an identity matrix (which you can verify in a similar manner), the matrix is unitary.


### Inner Product
---
TheÂ **inner product**Â is yet another important matrix operation that is only applied to vectors. Given two vectorsÂ ğ‘‰Â andÂ ğ‘ŠÂ of the same size, their inner productÂ $\langle V , W \rangle$Â is defined as a product of matricesÂ $V^\dagger$Â andÂ ğ‘Š:

$$\langle V , W \rangle = V^\dagger W$$

Let's break this down so it's a bit easier to understand. AÂ 1Ã—ğ‘›Â matrix (the adjoint of anÂ ğ‘›Ã—1Â vector) multiplied by anÂ ğ‘›Ã—1Â vector results in aÂ 1Ã—1Â matrix (which is equivalent to a scalar). The result of an inner product is that scalar.

That is, to calculate the inner product of two vectors, take the corresponding elements $V_k$Â andÂ $W_k$, multiply the complex conjugate ofÂ $V_k$Â byÂ $W_k$, and add up those products:

$$\langle V , W \rangle = \sum_{k=0}^{n-1}\overline{V_k}W_k$$

If you are familiar with theÂ **dot product**, you will notice that it is equivalent to inner product for real-numbered vectors.

>We use our definition for these tutorials because it matches the notation used in quantum computing. You might encounter other sources which define the inner product a little differently:Â $\langle V , W \rangle = W^\dagger V = V^T\overline{W}$, in contrast to theÂ $V^\dagger W$Â that we use. These definitions are almost equivalent, with some differences in the scalar multiplication by a complex number.

#### Vector Norm
---

An immediate application for the inner product is computing theÂ **vector norm**. The norm of vectorÂ ğ‘‰Â is defined asÂ $||V|| = \sqrt{\langle V , V \rangle}$. This condenses the vector down to a single non-negative real value. If the vector represents coordinates in space, the norm happens to be the length of the vector. A vector is calledÂ normalizedÂ if its norm is equal toÂ 1.

The inner product has the following properties:

- Distributivity over addition:Â <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>V</mi>
  <mo>+</mo>
  <mi>W</mi>
  <mo>,</mo>
  <mi>X</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
  <mo>=</mo>
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>V</mi>
  <mo>,</mo>
  <mi>X</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
  <mo>+</mo>
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>W</mi>
  <mo>,</mo>
  <mi>X</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
</math>Â andÂ <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>V</mi>
  <mo>,</mo>
  <mi>W</mi>
  <mo>+</mo>
  <mi>X</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
  <mo>=</mo>
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>V</mi>
  <mo>,</mo>
  <mi>W</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
  <mo>+</mo>
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>V</mi>
  <mo>,</mo>
  <mi>X</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
</math>
- Partial associativity with scalar multiplication:Â <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>x</mi>
  <mo>&#x22C5;</mo>
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>V</mi>
  <mo>,</mo>
  <mi>W</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
  <mo>=</mo>
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mover>
    <mi>x</mi>
    <mo accent="true">&#x2015;</mo>
  </mover>
  <mi>V</mi>
  <mo>,</mo>
  <mi>W</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
  <mo>=</mo>
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>V</mi>
  <mo>,</mo>
  <mi>x</mi>
  <mi>W</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
</math>
- Skew symmetry:Â <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>V</mi>
  <mo>,</mo>
  <mi>W</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
  <mo>=</mo>
  <mover>
    <mrow>
      <mo fence="false" stretchy="false">&#x27E8;</mo>
      <mi>W</mi>
      <mo>,</mo>
      <mi>V</mi>
      <mo fence="false" stretchy="false">&#x27E9;</mo>
    </mrow>
    <mo accent="true">&#x2015;</mo>
  </mover>
</math>
- Multiplying a vector by a unitary matrixÂ **preserves the vector's inner product with itself**Â (and therefore the vector's norm): <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>U</mi>
  <mi>V</mi>
  <mo>,</mo>
  <mi>U</mi>
  <mi>V</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
  <mo>=</mo>
  <mo fence="false" stretchy="false">&#x27E8;</mo>
  <mi>V</mi>
  <mo>,</mo>
  <mi>V</mi>
  <mo fence="false" stretchy="false">&#x27E9;</mo>
</math>

> Note that just like matrix multiplication, the inner product isÂ **not commutative**:Â âŸ¨ğ‘‰,ğ‘ŠâŸ©Â won't always equalÂ âŸ¨ğ‘Š,ğ‘‰âŸ©.

#### Outer Products
---

TheÂ **outer product**Â of two vectorsÂ ğ‘‰Â andÂ ğ‘ŠÂ is defined asÂ ğ‘‰ğ‘Šâ€ . That is, the outer product of anÂ ğ‘›Ã—1Â vector and anÂ ğ‘šÃ—1Â vector is anÂ ğ‘›Ã—ğ‘šÂ matrix. If we denote the outer product ofÂ ğ‘‰Â andÂ ğ‘ŠÂ asÂ ğ‘‹, thenÂ $X_{i,j} = V_i \cdot \overline{W_j}$.

### Tensor Products
---

TheÂ **tensor product**Â is a different way of multiplying matrices. Rather than multiplying rows by columns, the tensor product multiplies the second matrix by every element of the first matrix.

GivenÂ ğ‘›Ã—ğ‘šÂ matrixÂ ğ´Â andÂ ğ‘˜Ã—ğ‘™Â matrixÂ ğµ, their tensor productÂ ğ´âŠ—ğµÂ is anÂ (ğ‘›â‹…ğ‘˜)Ã—(ğ‘šâ‹…ğ‘™)Â matrix defined as follows:

$$A \otimes B =
\begin{bmatrix}
    A_{0,0} \cdot B & A_{0,1} \cdot B & \dotsb & A_{0,m-1} \cdot B \\
    A_{1,0} \cdot B & A_{1,1} \cdot B & \dotsb & A_{1,m-1} \cdot B \\
    \vdots & \vdots & \ddots & \vdots \\
    A_{n-1,0} \cdot B & A_{n-1,1} \cdot B & \dotsb & A_{n-1,m-1} \cdot B
\end{bmatrix} =$$
$$= \begin{bmatrix}
    A_{0,0} \cdot \begin{bmatrix}B_{0,0} & \dotsb & B_{0,l-1} \\ \vdots & \ddots & \vdots \\ B_{k-1,0} & \dotsb & b_{k-1,l-1} \end{bmatrix} & \dotsb &
    A_{0,m-1} \cdot \begin{bmatrix}B_{0,0} & \dotsb & B_{0,l-1} \\ \vdots & \ddots & \vdots \\ B_{k-1,0} & \dotsb & B_{k-1,l-1} \end{bmatrix} \\
    \vdots & \ddots & \vdots \\
    A_{n-1,0} \cdot \begin{bmatrix}B_{0,0} & \dotsb & B_{0,l-1} \\ \vdots & \ddots & \vdots \\ B_{k-1,0} & \dotsb & B_{k-1,l-1} \end{bmatrix} & \dotsb &
    A_{n-1,m-1} \cdot \begin{bmatrix}B_{0,0} & \dotsb & B_{0,l-1} \\ \vdots & \ddots & \vdots \\ B_{k-1,0} & \dotsb & B_{k-1,l-1} \end{bmatrix}
\end{bmatrix} =$$
$$= \begin{bmatrix}
    A_{0,0} \cdot B_{0,0} & \dotsb & A_{0,0} \cdot B_{0,l-1} & \dotsb & A_{0,m-1} \cdot B_{0,0} & \dotsb & A_{0,m-1} \cdot B_{0,l-1} \\
    \vdots & \ddots & \vdots & \dotsb & \vdots & \ddots & \vdots \\
    A_{0,0} \cdot B_{k-1,0} & \dotsb & A_{0,0} \cdot B_{k-1,l-1} & \dotsb & A_{0,m-1} \cdot B_{k-1,0} & \dotsb & A_{0,m-1} \cdot B_{k-1,l-1} \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
    A_{n-1,0} \cdot B_{0,0} & \dotsb & A_{n-1,0} \cdot B_{0,l-1} & \dotsb & A_{n-1,m-1} \cdot B_{0,0} & \dotsb & A_{n-1,m-1} \cdot B_{0,l-1} \\
    \vdots & \ddots & \vdots & \dotsb & \vdots & \ddots & \vdots \\
    A_{n-1,0} \cdot B_{k-1,0} & \dotsb & A_{n-1,0} \cdot B_{k-1,l-1} & \dotsb & A_{n-1,m-1} \cdot B_{k-1,0} & \dotsb & A_{n-1,m-1} \cdot B_{k-1,l-1}
\end{bmatrix}$$

Notice that the tensor product of two vectors is another vector: ifÂ ğ‘‰Â is anÂ ğ‘›Ã—1Â vector, andÂ ğ‘ŠÂ is anÂ ğ‘šÃ—1Â vector,Â ğ‘‰âŠ—ğ‘ŠÂ is anÂ (ğ‘›â‹…ğ‘š)Ã—1Â vector.

The tensor product has the following properties:

- Distributivity over addition:Â $(A + B) \otimes C = A \otimes C + B \otimes C$ , $A \otimes (B + C) = A \otimes B + A \otimes C$
- Associativity with scalar multiplication:Â $x(A \otimes B) = (xA) \otimes B = A \otimes (xB)$
- Mixed-product property (relation with matrix multiplication): $(A \otimes B) (C \otimes D) = (AC) \otimes (BD)$